apply plugin: 'java'
apply plugin: 'eclipse'
apply plugin: 'jacoco'
apply plugin: 'application'

compileJava.options.encoding = 'UTF-8'
sourceCompatibility = 1.8
version = '1.0'
jar {
    manifest {
        attributes 'Implementation-Title': 'Example - Spark 2 job (BitcoinBlock) for analysing Bitcoin data using hadoopcryptoledger', 'Implementation-Version': version
    }

    baseName = 'example-hcl-spark2-bitcoinblock'
    version = '0.1.0'
   // note this builds one fat jar and it is not recommended for production use - just for illustration purpose
   from { configurations.compile.collect { it.isDirectory() ? it : zipTree(it) } }
}

mainClassName = "org.zuinnote.spark.bitcoin.example.SparkBitcoinBlockCounter"

repositories {
    mavenCentral()
    mavenLocal()
}


jacocoTestReport {
    reports {
        xml.enabled true
        csv.enabled true
    }
}



configurations {
	provided
	testProvided
	testIntegrationCompile.extendsFrom testCompile
	testIntegrationRuntime.extendsFrom testRuntime
	all {
	  resolutionStrategy { // use Spark versions instead of Hadoop
            force 'io.netty:netty-all:4.1.17.Final', 'com.fasterxml.jackson.core:jackson-databind:2.6.7.1'
        }
	}
}

eclipse {

  classpath {
    plusConfigurations += [ configurations.provided ]
    plusConfigurations += [ configurations.testProvided ]
    plusConfigurations += [ configurations.testIntegrationCompile ]
	plusConfigurations += [ configurations.testIntegrationRuntime ]
  }
}

sourceSets {
    main.compileClasspath += configurations.provided
    test.compileClasspath += configurations.provided
    test.runtimeClasspath += configurations.provided
    testIntegration {
        java {
            compileClasspath += main.output + test.output + configurations.provided
            runtimeClasspath += main.output + test.output + configurations.provided
            srcDir file('src/integration-test/java')
        }
        resources.srcDir file('src/integration-test/resources')
	}
}

dependencies {

   // spark-core
    provided("org.apache.spark:spark-core_2.11:2.4.4")
   // spark-sql for SparkSession
 provided("org.apache.spark:spark-sql_2.11:2.4.4")
   // hadoop lib for input format
     provided("org.apache.hadoop:hadoop-client:2.10.0") 
   // hadoop crypto ledger library
   compile("com.github.zuinnote:hadoopcryptoledger-fileformat:1.2.1") 
        // testing
     testCompile group: 'org.junit.jupiter', name: 'junit-jupiter-api', version: '5.5.2'
     testRuntime group: 'org.junit.jupiter', name: 'junit-jupiter-engine', version: '5.5.2'
    testIntegrationRuntime group: 'org.junit.jupiter', name: 'junit-jupiter-engine', version: '5.5.2'
	testIntegrationRuntime group: 'org.junit.platform', name: 'junit-platform-console', version: '1.5.2'
    testIntegrationCompile group: 'javax.servlet', name: 'javax.servlet-api', version: '3.0.1' // need to avoid conflicts with different versions
    testIntegrationCompile group: 'org.apache.hadoop', name: 'hadoop-minicluster', version: '2.10.0'  
}

task testIntegration(type: Test) {
    testClassesDirs = sourceSets.testIntegration.output.classesDirs
    classpath = sourceSets.testIntegration.runtimeClasspath
    useJUnitPlatform()
}

check.dependsOn testIntegration
testIntegration.mustRunAfter test

uploadArchives {
    repositories {
       flatDir {
           dirs 'repos'
       }
    }
}
